{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9f77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Downloading data for: ['AAPL', 'MSFT', 'GOOGL']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Fitting GARCH models to filter volatility clusters...\n",
      "Step 3: Modeling fat tails and transforming to Uniform [0,1]...\n",
      "Step 4: Calibrating StudentCopula for tail dependence...\n",
      "              AAPL         MSFT        GOOGL\n",
      "count  1257.000000  1257.000000  1257.000000\n",
      "mean      0.500000     0.500000     0.500000\n",
      "std       0.288560     0.288560     0.288560\n",
      "min       0.000795     0.000795     0.000795\n",
      "25%       0.250397     0.250397     0.250397\n",
      "50%       0.500000     0.500000     0.500000\n",
      "75%       0.749603     0.749603     0.749603\n",
      "max       0.999205     0.999205     0.999205\n",
      "Step 5: Generating 100000 correlated scenarios...\n",
      "Scenarios generated. Proceeding to final price calculation...\n",
      "Step 6: Pricing the Worst-of Structured Product...\n",
      "\n",
      "========================================\n",
      "FINAL STRUCTURED PRODUCT RESULTS\n",
      "Underlying Assets: ['AAPL', 'MSFT', 'GOOGL']\n",
      "Protection Level (Strike): 90%\n",
      "Estimated Option Price: 0.0001%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "from scipy import stats\n",
    "\n",
    "# Updated Imports for the 'copulae' library\n",
    "from copulae import StudentCopula, GaussianCopula\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: DATA ACQUISITION\n",
    "# ==========================================\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "print(f\"Step 1: Downloading data for: {tickers}...\")\n",
    "data = yf.download(tickers, start=\"2019-01-01\", end=\"2024-01-01\")\n",
    "\n",
    "# Handle MultiIndex and extract Adjusted Close\n",
    "if 'Adj Close' in data.columns:\n",
    "    adj_close = data['Adj Close']\n",
    "else:\n",
    "    adj_close = data['Close']\n",
    "\n",
    "# Calculate log returns\n",
    "returns = np.log(adj_close / adj_close.shift(1)).dropna()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: VOLATILITY FILTERING (GARCH)\n",
    "# ==========================================\n",
    "print(\"Step 2: Fitting GARCH models to filter volatility clusters...\")\n",
    "std_residuals_dict = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Scale by 100 for numerical stability in the optimizer\n",
    "    model = arch_model(returns[ticker] * 100, vol='Garch', p=1, q=1, dist='Normal')\n",
    "    res = model.fit(disp='off')\n",
    "    # Standardized Residuals = (Actual Returns / Estimated Volatility)\n",
    "    std_residuals_dict[ticker] = res.resid / res.conditional_volatility\n",
    "\n",
    "cleaned_data = pd.DataFrame(std_residuals_dict)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: MARGINAL DISTRIBUTIONS (FAT TAILS)\n",
    "# ==========================================\n",
    "print(\"Step 3: Modeling fat tails and transforming to Uniform [0,1]...\")\n",
    "dist_params = {}\n",
    "uniform_data = pd.DataFrame()\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Fit Student-t distribution to the GARCH residuals\n",
    "    # Returns: (df, loc, scale)\n",
    "    params = stats.t.fit(cleaned_data[ticker])\n",
    "    dist_params[ticker] = params\n",
    "    \n",
    "    # Probability Integral Transform (PIT)\n",
    "    # This maps the residuals into the [0, 1] range needed for Copulas\n",
    "    uniform_data[ticker] = stats.t.cdf(cleaned_data[ticker], *params)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: COPULA CALIBRATION (CRASH CORRELATION)\n",
    "# ==========================================\n",
    "print(\"Step 4: Calibrating StudentCopula for tail dependence...\")\n",
    "# Initialize the copula. dim=3 for our 3 stocks.\n",
    "# The 'copulae' library uses 'StudentCopula'\n",
    "cop = StudentCopula(dim=len(tickers))\n",
    "\n",
    "# Fit the copula to your uniform data\n",
    "# This captures the hidden correlation between assets during market crashes\n",
    "cop.fit(uniform_data)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: MONTE CARLO SIMULATION\n",
    "# ==========================================\n",
    "num_sim = 100000\n",
    "print(f\"Step 5: Generating {num_sim} correlated scenarios...\")\n",
    "\n",
    "# FIX: Add .values to ensure we have a NumPy array for slicing\n",
    "sim_uniform = cop.random(num_sim).values \n",
    "\n",
    "sim_residuals = pd.DataFrame()\n",
    "for i, ticker in enumerate(tickers):\n",
    "    df, loc, scale = dist_params[ticker]\n",
    "    # Now sim_uniform[:, i] will work correctly\n",
    "    sim_residuals[ticker] = stats.t.ppf(sim_uniform[:, i], df, loc, scale)\n",
    "\n",
    "print(\"Scenarios generated. Proceeding to final price calculation...\")\n",
    "\n",
    "# Project future prices for a 1-year horizon\n",
    "current_prices = adj_close.iloc[-1]\n",
    "future_prices = current_prices * np.exp(sim_residuals / 100) # Reversing GARCH scale\n",
    "\n",
    "# ==========================================\n",
    "# STEP 6: PRICING THE WORST-OF OPTION\n",
    "# ==========================================\n",
    "print(\"Step 6: Pricing the Worst-of Structured Product...\")\n",
    "# Performance = (Future Price / Current Price)\n",
    "sim_performance = future_prices / current_prices\n",
    "\n",
    "# The 'Worst-of' is the minimum performance in each simulation\n",
    "worst_of_perf = sim_performance.min(axis=1)\n",
    "\n",
    "# Define product: Pays the investor if the worst stock drops below 90%\n",
    "strike = 0.90\n",
    "payoffs = np.maximum(0, strike - worst_of_perf)\n",
    "\n",
    "# The fair price is the average payoff across all simulations\n",
    "price_estimate = payoffs.mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"FINAL STRUCTURED PRODUCT RESULTS\")\n",
    "print(f\"Underlying Assets: {tickers}\")\n",
    "print(f\"Protection Level (Strike): {strike:.0%}\")\n",
    "print(f\"Estimated Option Price: {price_estimate:.4%}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad894a7-6acd-432d-9353-5e6b0bc684dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
